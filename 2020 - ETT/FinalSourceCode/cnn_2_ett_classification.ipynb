{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cnn_2_ett_classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqhU8LELstO6",
        "cellView": "code"
      },
      "source": [
        "# Mount google drive to download crops.zip that contains 256x128 cropped images\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkbDI5YbPKJ9"
      },
      "source": [
        "# Prepare the environment before training the model\n",
        "!unzip -q \"/content/drive/My Drive/Colab Notebooks/crops.zip\"\n",
        "!cp \"/content/drive/My Drive/Colab Notebooks/truePos.csv\" truePos.csv\n",
        "!cp \"/content/drive/My Drive/Colab Notebooks/trueNeg.csv\" trueNeg.csv\n",
        "!cp \"/content/drive/My Drive/Colab Notebooks/ett.csv\" ett.csv\n",
        "!cp \"/content/drive/My Drive/Colab Notebooks/db-mimic-frontal-only.csv\" db-mimic-frontal-only.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hv0EF9j7Aqx"
      },
      "source": [
        "# Import libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow import losses, optimizers\n",
        "from tensorflow.keras import Input, Model, models, layers, backend\n",
        "import numpy as np, pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1F84JppS62Pb"
      },
      "source": [
        "# Create the images and labels for training and testing\n",
        "import glob #file operations\n",
        "import pickle as pkl #save/load arrays into/from files\n",
        "import numpy as np #matrix operations\n",
        "import pdb #python debugging\n",
        "\n",
        "def create_train_test_data(crossValFold):\n",
        "  # Initialize variables\n",
        "  images_train = []\n",
        "  labels_train = []\n",
        "  images_test = []\n",
        "  labels_test = []\n",
        "\n",
        "  # Search for all image files recursively\n",
        "  imageFiles = glob.glob('./crops/**/*.npy', recursive=True)\n",
        "\n",
        "  # Read the contents of the true positive file\n",
        "  with open(\"./truePos.csv\", 'r') as truePosFile:\n",
        "    truePosContent = truePosFile.read()\n",
        "\n",
        "  # Read the contents of the true negative file\n",
        "  with open(\"./trueNeg.csv\", 'r') as trueNegFile:\n",
        "    trueNegContent = trueNegFile.read()\n",
        "\n",
        "  # Read the contents of the ett file\n",
        "  with open(\"./ett.csv\", 'r') as ettFile:\n",
        "    ettContent = ettFile.read()\n",
        "\n",
        "  # Read the contents of the frontal file\n",
        "  with open(\"./db-mimic-frontal-only.csv\", 'r') as frontalFile:\n",
        "    frontalContent = frontalFile.read()\n",
        "\n",
        "  # For each cross validation fold, 2 folders are used for test and 8 folders for train\n",
        "  testFolder1 = \"/p\" + str(10+2*(crossValFold-1)) + \"/\"\n",
        "  testFolder2 = \"/p\" + str(11+2*(crossValFold-1)) + \"/\"\n",
        "  print(\"Test Folders: \" + testFolder1.replace(\"/\", \"\") + \"-\" + testFolder2.replace(\"/\", \"\") + \"\\n\")\n",
        "\n",
        "  # Process each image file\n",
        "  for imageFile in imageFiles:\n",
        "    # Load image into an array\n",
        "    img = np.load(imageFile)\n",
        "\n",
        "    # Remove single-dimensional entries from the image array\n",
        "    img = np.squeeze(img)\n",
        "\n",
        "    # Skip the shapes that are not in the form of (256, 128)\n",
        "    if img.shape[0] != 256 or img.shape[1] != 128:\n",
        "      continue\n",
        "\n",
        "    # Get image ID from image file path\n",
        "    imgStart = imageFile.rfind(\"/\") + 1\n",
        "    imgEnd = imageFile.rfind(\".npy\")\n",
        "    imgID = imageFile[imgStart:imgEnd]\n",
        "\n",
        "    # Skip the image if it's not frontal\n",
        "    if frontalContent.find(imgID) == -1:\n",
        "      continue\n",
        "\n",
        "    # Get patient ID from image file path\n",
        "    patientStart = imageFile.rfind(\"/p\") + 2\n",
        "    patientEnd = imageFile.rfind(\"/s\")\n",
        "    patientID = imageFile[patientStart:patientEnd]\n",
        "\n",
        "    # Get study ID from image file path\n",
        "    studyStart = imageFile.rfind(\"/s\") + 2\n",
        "    studyEnd = imageFile.rfind(\"/\")\n",
        "    studyID = imageFile[studyStart:studyEnd]\n",
        "\n",
        "    # Search for image in true positive file\n",
        "    imgStart = truePosContent.find(imgID)\n",
        "    if imgStart == -1:\n",
        "      # Search for image in true negative file\n",
        "      imgStart = trueNegContent.find(imgID)\n",
        "      if imgStart == -1:\n",
        "        # Search for study in ETT file\n",
        "        imgStart = ettContent.find(patientID + \",\" + studyID)\n",
        "      else:\n",
        "        imgStart = -1\n",
        "\n",
        "    # Check image path for train or test\n",
        "    if testFolder1 in imageFile or testFolder2 in imageFile:\n",
        "      # Append to test images in flattened form to hold in one row\n",
        "      images_test.append(img.flatten())\n",
        "\n",
        "      # Append to test labels based on ETT search\n",
        "      if imgStart > -1:\n",
        "        labels_test.append(1)\n",
        "      else:\n",
        "        labels_test.append(0)\n",
        "    else:\n",
        "      # Append to train images in flattened form to hold in one row\n",
        "      images_train.append(img.flatten())\n",
        "\n",
        "      # Append to train labels based on ETT search\n",
        "      if imgStart > -1:\n",
        "        labels_train.append(1)\n",
        "      else:\n",
        "        labels_train.append(0)\n",
        "\n",
        "  # Convert lists to np arrays\n",
        "  x_train = np.array(images_train)\n",
        "  y_train = np.array(labels_train)\n",
        "  x_test = np.array(images_test)\n",
        "  y_test = np.array(labels_test)\n",
        "\n",
        "  # Print training image count\n",
        "  print(\"Train Total: \" + str(len(y_train)))\n",
        "  print(\"Train Positive: \" + str(np.count_nonzero(y_train)))\n",
        "  print(\"Train Negative: \" + str(np.count_nonzero(y_train==0)) + \"\\n\")\n",
        "\n",
        "  # Print test image count\n",
        "  print(\"Test Total: \" + str(len(y_test)))\n",
        "  print(\"Test Positive: \" + str(np.count_nonzero(y_test)))\n",
        "  print(\"Test Negative: \" + str(np.count_nonzero(y_test==0)) + \"\\n\")\n",
        "\n",
        "  return x_train, y_train, x_test, y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5h4oxNzjENrO"
      },
      "source": [
        "# Create the images and labels for training only to build the full model\n",
        "import glob #file operations\n",
        "import pickle as pkl #save/load arrays into/from files\n",
        "import numpy as np #matrix operations\n",
        "import pdb #python debugging\n",
        "\n",
        "def create_train_data():\n",
        "  # Initialize variables\n",
        "  images_train = []\n",
        "  labels_train = []\n",
        "\n",
        "  # Search for all image files recursively\n",
        "  imageFiles = glob.glob('./crops/**/*.npy', recursive=True)\n",
        "\n",
        "  # Read the contents of the true positive file\n",
        "  with open(\"./truePos.csv\", 'r') as truePosFile:\n",
        "    truePosContent = truePosFile.read()\n",
        "\n",
        "  # Read the contents of the true negative file\n",
        "  with open(\"./trueNeg.csv\", 'r') as trueNegFile:\n",
        "    trueNegContent = trueNegFile.read()\n",
        "\n",
        "  # Read the contents of the ett file\n",
        "  with open(\"./ett.csv\", 'r') as ettFile:\n",
        "    ettContent = ettFile.read()\n",
        "\n",
        "  # Read the contents of the frontal file\n",
        "  with open(\"./db-mimic-frontal-only.csv\", 'r') as frontalFile:\n",
        "    frontalContent = frontalFile.read()\n",
        "\n",
        "  # For each cross validation fold, 2 folders are used for test and 8 folders for train\n",
        "  testFolder1 = \"/p\" + str(10+2*(crossValFold-1)) + \"/\"\n",
        "  testFolder2 = \"/p\" + str(11+2*(crossValFold-1)) + \"/\"\n",
        "  print(\"Test Folders: \" + testFolder1.replace(\"/\", \"\") + \"-\" + testFolder2.replace(\"/\", \"\") + \"\\n\")\n",
        "\n",
        "  # Process each image file\n",
        "  for imageFile in imageFiles:\n",
        "    # Load image into an array\n",
        "    img = np.load(imageFile)\n",
        "\n",
        "    # Remove single-dimensional entries from the image array\n",
        "    img = np.squeeze(img)\n",
        "\n",
        "    # Skip the shapes that are not in the form of (256, 128)\n",
        "    if img.shape[0] != 256 or img.shape[1] != 128:\n",
        "      continue\n",
        "\n",
        "    # Get image ID from image file path\n",
        "    imgStart = imageFile.rfind(\"/\") + 1\n",
        "    imgEnd = imageFile.rfind(\".npy\")\n",
        "    imgID = imageFile[imgStart:imgEnd]\n",
        "\n",
        "    # Skip the image if it's not frontal\n",
        "    if frontalContent.find(imgID) == -1:\n",
        "      continue\n",
        "\n",
        "    # Get patient ID from image file path\n",
        "    patientStart = imageFile.rfind(\"/p\") + 2\n",
        "    patientEnd = imageFile.rfind(\"/s\")\n",
        "    patientID = imageFile[patientStart:patientEnd]\n",
        "\n",
        "    # Get study ID from image file path\n",
        "    studyStart = imageFile.rfind(\"/s\") + 2\n",
        "    studyEnd = imageFile.rfind(\"/\")\n",
        "    studyID = imageFile[studyStart:studyEnd]\n",
        "\n",
        "    # Search for image in true positive file\n",
        "    imgStart = truePosContent.find(imgID)\n",
        "    if imgStart == -1:\n",
        "      # Search for image in true negative file\n",
        "      imgStart = trueNegContent.find(imgID)\n",
        "      if imgStart == -1:\n",
        "        # Search for study in ETT file\n",
        "        imgStart = ettContent.find(patientID + \",\" + studyID)\n",
        "      else:\n",
        "        imgStart = -1\n",
        "\n",
        "    # Append to train images in flattened form to hold in one row\n",
        "    images_train.append(img.flatten())\n",
        "\n",
        "    # Append to train labels based on ETT search\n",
        "    if imgStart > -1:\n",
        "      labels_train.append(1)\n",
        "    else:\n",
        "      labels_train.append(0)\n",
        "\n",
        "  # Convert lists to np arrays\n",
        "  x_train = np.array(images_train)\n",
        "  y_train = np.array(labels_train)\n",
        "\n",
        "  # Print training image count\n",
        "  print(\"Train Total: \" + str(len(y_train)))\n",
        "  print(\"Train Positive: \" + str(np.count_nonzero(y_train)))\n",
        "  print(\"Train Negative: \" + str(np.count_nonzero(y_train==0)) + \"\\n\")\n",
        "\n",
        "  return x_train, y_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flybkN5NcP6_"
      },
      "source": [
        "# How many positive/negative images\n",
        "import numpy as np\n",
        "import pylab as py\n",
        "\n",
        "# Create train and test data for the first cross validation fold\n",
        "x_train, y_train, x_test, y_test = create_train_test_data(1)\n",
        "\n",
        "# Print 16 random images\n",
        "fig = py.figure()\n",
        "for i in range(16):\n",
        "  imIndex = np.random.randint(x_train.shape[0])\n",
        "  im = x_train[imIndex].reshape(256, 128)\n",
        "  fig.add_subplot(4, 4, i + 1)\n",
        "  py.imshow(im)\n",
        "  py.axis('off')\n",
        "  print(y_train[imIndex])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m77hXM0S8J8g"
      },
      "source": [
        "# --- Prepare\n",
        "def prepare_model(inputs):\n",
        "  # --- Define kwargs dictionary\n",
        "  kwargs = {'kernel_size': (3, 3), 'padding': 'same'}\n",
        "\n",
        "  # --- Define lambda functions\n",
        "  conv = lambda x, filters, strides : layers.Conv2D(filters=filters, strides=strides, **kwargs)(x)\n",
        "  norm = lambda x : layers.BatchNormalization()(x)\n",
        "  relu = lambda x : layers.LeakyReLU()(x)\n",
        "\n",
        "  # --- Define stride-1, stride-2 blocks\n",
        "  conv1 = lambda filters, x : relu(norm(conv(x, filters, strides=1)))\n",
        "  conv2 = lambda filters, x : relu(norm(conv(x, filters, strides=(2, 2))))\n",
        "\n",
        "  # --- Define contracting layers\n",
        "  l1 = conv2(48, conv1(48, conv1(48, conv1(48, conv1(48, conv1(48, inputs['dat']))))))\n",
        "  l2 = conv2(56, conv1(56, conv1(56, conv1(56, conv1(56, l1)))))\n",
        "  l3 = conv2(64, conv1(64, conv1(64, conv1(64, conv1(64, l2)))))\n",
        "  l4 = conv2(80, conv1(80, conv1(80, conv1(80, l3))))\n",
        "  l5 = conv2(96, conv1(96, conv1(96, conv1(96, l4))))\n",
        "  l6 = conv2(112, conv1(112, conv1(112, l5)))\n",
        "  l7 = conv2(128, conv1(128, conv1(128, l6)))\n",
        "\n",
        "  # --- Flatten\n",
        "  f0 = layers.Flatten()(l7)\n",
        "\n",
        "  # --- Create logits\n",
        "  logits = {}\n",
        "  logits['ett'] = layers.Dense(2, name='ett')(f0)\n",
        "\n",
        "  # --- Create model\n",
        "  model = Model(inputs=inputs, outputs=logits) \n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEDC8wdV_IMA"
      },
      "source": [
        "# --- Validate\n",
        "import math\n",
        "import datetime\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def test_model(fold, x_test, y_test):\n",
        "  # Set test total values\n",
        "  tpCnt = 0\n",
        "  tnCnt = 0\n",
        "  fpCnt = 0\n",
        "  fnCnt = 0\n",
        "  totalCnt = len(y_test)\n",
        "\n",
        "  # Set start time\n",
        "  startTime = datetime.datetime.now().time()\n",
        "\n",
        "  # Predict\n",
        "  logits = model.predict(x=x_test.reshape(x_test.shape[0], 256, 128, 1))\n",
        "\n",
        "  # Convert logits to predictions\n",
        "  if type(logits) is dict:\n",
        "      logits = logits['ett']\n",
        "  predictions = np.argmax(logits, axis=1)\n",
        "\n",
        "  # Set end time\n",
        "  endTime = datetime.datetime.now().time()\n",
        "\n",
        "  # Print start and end times\n",
        "  print(\"\\nStart Time: \" + str(startTime))\n",
        "  print(\"End Time: \" + str(endTime))\n",
        "  print(\"Item Count: \" + str(totalCnt))\n",
        "\n",
        "  # Keep probabilities for the positive outcome only\n",
        "  probs = logits[:, 1]\n",
        "\n",
        "  # Calculate ROC AUC score\n",
        "  auc = roc_auc_score(y_test, probs)\n",
        "  print('\\nROC AUC=%.3f' % (auc))\n",
        "\n",
        "  # List the negative predictions\n",
        "  negIndices = np.where(predictions != y_test)\n",
        "  for i in negIndices[0]:\n",
        "    if predictions[i] == 1:\n",
        "      fpCnt += 1\n",
        "    else:\n",
        "      fnCnt += 1\n",
        "\n",
        "  # List the positive predictions\n",
        "  posIndices = np.where(predictions == y_test)\n",
        "  for i in posIndices[0]:\n",
        "    if predictions[i] == 1:\n",
        "      tpCnt += 1\n",
        "    else:\n",
        "      tnCnt += 1\n",
        "\n",
        "  # Print TP, TN, FP, and FN\n",
        "  print(\"\\nTP: \" + str(tpCnt) + \" TN: \" + str(tnCnt) + \" FP: \" + str(fpCnt) + \" FN: \" + str(fnCnt) + \"\\n\")\n",
        "\n",
        "  return tpCnt, tnCnt, fpCnt, fnCnt, auc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivw7jsCCHL9S"
      },
      "source": [
        "#--------------------------------------------\n",
        "# Train and test with 5-fold cross validation\n",
        "#--------------------------------------------\n",
        "\n",
        "# Initialize total counts\n",
        "tpTotal = 0\n",
        "tnTotal = 0\n",
        "fpTotal = 0\n",
        "fnTotal = 0\n",
        "aucTotal = 0\n",
        "\n",
        "# Loop for 5-fold cross validation\n",
        "for fold in range(5):\n",
        "  print('fold: ' + str(fold))\n",
        "\n",
        "  # Create model inputs\n",
        "  inputs = {}\n",
        "  inputs['dat'] = Input(shape=(256, 128, 1))\n",
        "\n",
        "  # Prepare the model\n",
        "  model = prepare_model(inputs)\n",
        "\n",
        "  # Create train and test data for the cross validation fold\n",
        "  x_train, y_train, x_test, y_test = create_train_test_data(fold + 1)\n",
        "\n",
        "  # Initialize learning rate and epoch\n",
        "  lr = 0.0005\n",
        "  epoch = 1\n",
        "\n",
        "  # Compile the model\n",
        "  model.compile(\n",
        "      optimizer=optimizers.Adam(learning_rate=lr),\n",
        "      loss={'ett': losses.SparseCategoricalCrossentropy(from_logits=True)}, \n",
        "      metrics={'ett': 'sparse_categorical_accuracy'})\n",
        "\n",
        "  for i in range(3):\n",
        "    # Print learning-rate and epoch\n",
        "    print('learning-rate: ' + str(lr))\n",
        "    print('epoch: ' + str(epoch))\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(\n",
        "        x=x_train.reshape(x_train.shape[0], 256, 128, 1), \n",
        "        y=y_train,\n",
        "        batch_size=12,\n",
        "        steps_per_epoch=None, \n",
        "        epochs=1)\n",
        "\n",
        "    # Increment epoch\n",
        "    epoch += 1\n",
        "\n",
        "  # Set learning rate\n",
        "  lr = 0.00005\n",
        "\n",
        "  # Compile the model\n",
        "  model.compile(\n",
        "      optimizer=optimizers.Adam(learning_rate=lr),\n",
        "      loss={'ett': losses.SparseCategoricalCrossentropy(from_logits=True)}, \n",
        "      metrics={'ett': 'sparse_categorical_accuracy'})\n",
        "\n",
        "  # Print learning-rate and epoch\n",
        "  print('learning-rate: ' + str(lr))\n",
        "  print('epoch: ' + str(epoch))\n",
        "\n",
        "  # Train the model\n",
        "  model.fit(\n",
        "      x=x_train.reshape(x_train.shape[0], 256, 128, 1), \n",
        "      y=y_train,\n",
        "      batch_size=12,\n",
        "      steps_per_epoch=None, \n",
        "      epochs=1)\n",
        "\n",
        "  # Increment epoch\n",
        "  epoch += 1\n",
        "\n",
        "  # Set learning rate\n",
        "  lr = 0.000005\n",
        "\n",
        "  # Compile the model\n",
        "  model.compile(\n",
        "      optimizer=optimizers.Adam(learning_rate=lr),\n",
        "      loss={'ett': losses.SparseCategoricalCrossentropy(from_logits=True)}, \n",
        "      metrics={'ett': 'sparse_categorical_accuracy'})\n",
        "\n",
        "  # Print learning-rate and epoch\n",
        "  print('learning-rate: ' + str(lr))\n",
        "  print('epoch: ' + str(epoch))\n",
        "\n",
        "  # Train the model\n",
        "  model.fit(\n",
        "      x=x_train.reshape(x_train.shape[0], 256, 128, 1), \n",
        "      y=y_train,\n",
        "      batch_size=12,\n",
        "      steps_per_epoch=None, \n",
        "      epochs=1)\n",
        "\n",
        "  # Test the model\n",
        "  tpCnt, tnCnt, fpCnt, fnCnt, auc = test_model(fold, x_test, y_test)\n",
        "\n",
        "  # Add to totals\n",
        "  tpTotal += tpCnt\n",
        "  tnTotal += tnCnt\n",
        "  fpTotal += fpCnt\n",
        "  fnTotal += fnCnt\n",
        "  aucTotal += auc\n",
        "\n",
        "  # Delete the model at the end of each fold\n",
        "  del model\n",
        "  del x_train\n",
        "  del y_train\n",
        "  del x_test\n",
        "  del y_test\n",
        "\n",
        "# Print Total TP, TN, FP, and FN\n",
        "print(\"Total TP: \" + str(tpTotal) + \" Total TN: \" + str(tnTotal) + \" Total FP: \" + str(fpTotal) + \" Total FN: \" + str(fnTotal) + \"\\n\")\n",
        "print(\"Calculate accuracy, sensitivity, and specificity by using these values at https://www.medcalc.org/calc/diagnostic_test.php\\n\")\n",
        "print(\"Avg AUC: \" + str(aucTotal/5) + \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYQeBn0AySBk"
      },
      "source": [
        "#--------------------------------------------------\n",
        "# Generate a model file from the whole training set\n",
        "#--------------------------------------------------\n",
        "\n",
        "# Create model inputs\n",
        "inputs = {}\n",
        "inputs['dat'] = Input(shape=(256, 128, 1))\n",
        "\n",
        "# Prepare the model\n",
        "model = prepare_model(inputs)\n",
        "\n",
        "# Create train data to build a model\n",
        "x_train, y_train = create_train_data()\n",
        "\n",
        "# Initialize learning rate and epoch\n",
        "lr = 0.0005\n",
        "epoch = 1\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer=optimizers.Adam(learning_rate=lr),\n",
        "    loss={'ett': losses.SparseCategoricalCrossentropy(from_logits=True)}, \n",
        "    metrics={'ett': 'sparse_categorical_accuracy'})\n",
        "\n",
        "for i in range(3):\n",
        "  # Print learning-rate and epoch\n",
        "  print('learning-rate: ' + str(lr))\n",
        "  print('epoch: ' + str(epoch))\n",
        "\n",
        "  # Train the model\n",
        "  model.fit(\n",
        "      x=x_train.reshape(x_train.shape[0], 256, 128, 1), \n",
        "      y=y_train,\n",
        "      batch_size=12,\n",
        "      steps_per_epoch=None, \n",
        "      epochs=1)\n",
        "\n",
        "  # Increment epoch\n",
        "  epoch += 1\n",
        "\n",
        "# Set learning rate\n",
        "lr = 0.00005\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer=optimizers.Adam(learning_rate=lr),\n",
        "    loss={'ett': losses.SparseCategoricalCrossentropy(from_logits=True)}, \n",
        "    metrics={'ett': 'sparse_categorical_accuracy'})\n",
        "\n",
        "# Print learning-rate and epoch\n",
        "print('learning-rate: ' + str(lr))\n",
        "print('epoch: ' + str(epoch))\n",
        "\n",
        "# Train the model\n",
        "model.fit(\n",
        "    x=x_train.reshape(x_train.shape[0], 256, 128, 1), \n",
        "    y=y_train,\n",
        "    batch_size=12,\n",
        "    steps_per_epoch=None, \n",
        "    epochs=1)\n",
        "\n",
        "# Increment epoch\n",
        "epoch += 1\n",
        "\n",
        "# Set learning rate\n",
        "lr = 0.000005\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer=optimizers.Adam(learning_rate=lr),\n",
        "    loss={'ett': losses.SparseCategoricalCrossentropy(from_logits=True)}, \n",
        "    metrics={'ett': 'sparse_categorical_accuracy'})\n",
        "\n",
        "# Print learning-rate and epoch\n",
        "print('learning-rate: ' + str(lr))\n",
        "print('epoch: ' + str(epoch))\n",
        "\n",
        "# Train the model\n",
        "model.fit(\n",
        "    x=x_train.reshape(x_train.shape[0], 256, 128, 1), \n",
        "    y=y_train,\n",
        "    batch_size=12,\n",
        "    steps_per_epoch=None, \n",
        "    epochs=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ep1O5jVT3k1e"
      },
      "source": [
        "# --- Save the model to a file\n",
        "model.save('./cnn_2_ett_classification.hdf5')\n",
        "\n",
        "# Copy the model to google drive\n",
        "!cp ./cnn_2_ett_classification.hdf5 '/content/drive/My Drive/Colab Notebooks/cnn_2_ett_classification.hdf5'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbcD_pPP3xo9"
      },
      "source": [
        "# Copy the model file from google drive\n",
        "!cp '/content/drive/My Drive/Colab Notebooks/cnn_2_ett_classification.hdf5' ./cnn_2_ett_classification.hdf5\n",
        "\n",
        "# Load the model from the file\n",
        "from tensorflow.keras import models as tfModels\n",
        "model = tfModels.load_model('./cnn_2_ett_classification.hdf5', compile=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}