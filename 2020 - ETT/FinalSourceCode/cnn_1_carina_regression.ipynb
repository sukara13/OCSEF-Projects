{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cnn_1_carina_regression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "GI91UpjS0LsU"
      },
      "source": [
        "# Mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAZxxPwL4DFi"
      },
      "source": [
        "# Copy annotation and report list files from google drive\n",
        "!cp \"/content/drive/My Drive/Colab Notebooks/new_carina_annotations_7396.csv\" \"./new_carina_annotations_7396.csv\"\n",
        "!cp \"/content/drive/My Drive/Colab Notebooks/cxr-record-list.csv\" \"./cxr-record-list.csv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "_NuuXXPpvWFz"
      },
      "source": [
        "# Download mimic.zip file that contains 512xN preprocessed images\n",
        "!wget -O mimic.zip \"removed due to privacy requirements of MIMIC-CXR providers\"\n",
        "!unzip -q mimic.zip # Unzip mimic.zip file\n",
        "!rm mimic.zip #remove zip file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OylAMrqUVGb"
      },
      "source": [
        "# Import libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow import losses, optimizers\n",
        "from tensorflow.keras import Input, Model, models, layers, backend\n",
        "import numpy as np, pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JiH_JEYIbXI3"
      },
      "source": [
        "# Find annotated images in npys and save them into the annotated images folder\n",
        "import glob\n",
        "import shutil\n",
        "import numpy as np\n",
        "\n",
        "# Create directory for annotated images\n",
        "!mkdir 'carina_annotated_images'\n",
        "\n",
        "# Search for all image files recursively\n",
        "imgFiles = glob.glob('./npys/**/*.npy', recursive=True)\n",
        "\n",
        "# Read the lines in the annotations file\n",
        "with open(\"./new_carina_annotations_7396.csv\", 'r') as antFile:\n",
        "  antLines = antFile.readlines()\n",
        "\n",
        "# Process each annotation\n",
        "for antLine in antLines:\n",
        "  # Search for an image\n",
        "  imgEnd = antLine.find(\",\")\n",
        "\n",
        "  # Get image name\n",
        "  imgName = antLine[:imgEnd]\n",
        "\n",
        "  # Find and copy the image file in the annotated images folder\n",
        "  for i, elem in enumerate(imgFiles):\n",
        "    if imgName in elem:\n",
        "      shutil.copyfile(imgFiles[i], \"./carina_annotated_images/\" + imgName + \".npy\")\n",
        "      break\n",
        "\n",
        "# Zip the carina_annotated_images folder\n",
        "!zip -q carina_annotated_images_7396.zip ./carina_annotated_images/*.*\n",
        "\n",
        "# Copy the zip file into google drive\n",
        "!cp ./carina_annotated_images_7396.zip \"/content/drive/My Drive/Colab Notebooks/carina_annotated_images_7396.zip\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5HrQUzvOLF3"
      },
      "source": [
        "# Once the annotated image zip is saved, no need to download mimic.zip file anymore\n",
        "# Just unzip the annotated images from google drive\n",
        "!unzip -q \"/content/drive/My Drive/Colab Notebooks/carina_annotated_images_7396.zip\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1F84JppS62Pb"
      },
      "source": [
        "# Create the images and labels for training and testing\n",
        "import glob #file operations\n",
        "import pickle as pkl #save/load arrays into/from files\n",
        "import numpy as np #matrix operations\n",
        "import pdb #python debugging\n",
        "import h5py\n",
        "\n",
        "def create_train_test_data(fold_count, fold_index):\n",
        "  # Initialize variables\n",
        "  images_train = []\n",
        "  labels_train = []\n",
        "  images_test = []\n",
        "  labels_test = []\n",
        "\n",
        "  # Search for all image files recursively\n",
        "  imageFiles = glob.glob('./carina_annotated_images/*.npy', recursive=False)\n",
        "\n",
        "  # Set counts\n",
        "  imgCnt = len(imageFiles)\n",
        "  testCnt = int(imgCnt / fold_count)\n",
        "\n",
        "  # Read the contents of the annotations file\n",
        "  with open(\"./new_carina_annotations_7396.csv\", 'r') as annotationsFile:\n",
        "    annotations = annotationsFile.read()\n",
        "\n",
        "  # Process each image file\n",
        "  imgIndex = 0\n",
        "  for imageFile in imageFiles:\n",
        "    # Load image into an array\n",
        "    img = np.load(imageFile)\n",
        "\n",
        "    # Remove single-dimensional entries from the image array\n",
        "    img = np.squeeze(img)\n",
        "\n",
        "    # Copy into an empty 512x512 image\n",
        "    emptyImg = np.zeros((512,512), np.uint16)\n",
        "    emptyImg[0:img.shape[0], 0:img.shape[1]] = img\n",
        "\n",
        "    # Assign the new image back to original\n",
        "    img = emptyImg\n",
        "\n",
        "    # Get image ID from image file path\n",
        "    imgStart = imageFile.rfind(\"/\") + 1\n",
        "    imgEnd = imageFile.rfind(\".npy\")\n",
        "    imgID = imageFile[imgStart:imgEnd]\n",
        "\n",
        "    # Search for image in annotations\n",
        "    imgStart = annotations.find(imgID)\n",
        "    if imgStart != -1:\n",
        "      imgStart = annotations.find(\",\", imgStart) + 1\n",
        "\n",
        "      # Get y coordinate\n",
        "      yStart = imgStart\n",
        "      yEnd = annotations.find(\",\", yStart)\n",
        "      y = annotations[yStart:yEnd]\n",
        "\n",
        "      # Get x coordinate\n",
        "      xStart = yEnd + 1\n",
        "      xEnd = annotations.find(\"\\n\", xStart)\n",
        "      x = annotations[xStart:xEnd]\n",
        "\n",
        "      # For each cross validation fold, 20% of images are used for test and 80% for train\n",
        "      if imgIndex >= testCnt*(fold_index-1) and imgIndex < testCnt*fold_index:\n",
        "        # Append to test images in flattened form to hold in one row\n",
        "        images_test.append(img.flatten())\n",
        "\n",
        "        # Append to test labels\n",
        "        labels_test.append([y,x])\n",
        "      else:\n",
        "        # Append to train images in flattened form to hold in one row\n",
        "        images_train.append(img.flatten())\n",
        "\n",
        "        # Append to train labels\n",
        "        labels_train.append([y,x])\n",
        "\n",
        "      imgIndex += 1\n",
        "  \n",
        "  # Convert lists to np arrays and normalize\n",
        "  x_train = np.array(images_train).astype(np.float32)\n",
        "  y_train = np.array(labels_train).astype(np.float32)\n",
        "  for i in range (len(y_train)):\n",
        "    x_train[i] = (x_train[i] - np.mean(x_train[i])) / np.std(x_train[i])\n",
        "    y_train[i] /= 511\n",
        "\n",
        "  x_test = np.array(images_test).astype(np.float32)\n",
        "  y_test = np.array(labels_test).astype(np.float32)\n",
        "  for i in range (len(y_test)):\n",
        "    x_test[i] = (x_test[i] - np.mean(x_test[i])) / np.std(x_test[i])\n",
        "    y_test[i] /= 511\n",
        "\n",
        "  del images_train\n",
        "  del labels_train\n",
        "  del images_test\n",
        "  del labels_test\n",
        "\n",
        "  # Print training image count\n",
        "  print(\"Train Total: \" + str(len(y_train)))\n",
        "\n",
        "  # Print test image count\n",
        "  print(\"Test Total: \" + str(len(y_test)))\n",
        "\n",
        "  return x_train, y_train, x_test, y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8u4Qo-Ozt9Q"
      },
      "source": [
        "# Create the images and labels to build a model\n",
        "import glob #file operations\n",
        "import pickle as pkl #save/load arrays into/from files\n",
        "import numpy as np #matrix operations\n",
        "import pdb #python debugging\n",
        "import h5py\n",
        "\n",
        "def create_train_data():\n",
        "  # Initialize variables\n",
        "  images_train = []\n",
        "  labels_train = []\n",
        "\n",
        "  # Search for all image files recursively\n",
        "  imageFiles = glob.glob('./carina_annotated_images/*.npy', recursive=False)\n",
        "\n",
        "  # Read the contents of the annotations file\n",
        "  with open(\"./new_carina_annotations_7396.csv\", 'r') as annotationsFile:\n",
        "    annotations = annotationsFile.read()\n",
        "\n",
        "  # Process each image file\n",
        "  imgIndex = 0\n",
        "  for imageFile in imageFiles:\n",
        "    # Load image into an array\n",
        "    img = np.load(imageFile)\n",
        "\n",
        "    # Remove single-dimensional entries from the image array\n",
        "    img = np.squeeze(img)\n",
        "\n",
        "    # Copy into an empty 512x512 image\n",
        "    emptyImg = np.zeros((512,512), np.uint16)\n",
        "    emptyImg[0:img.shape[0], 0:img.shape[1]] = img\n",
        "\n",
        "    # Assign the new image back to original\n",
        "    img = emptyImg\n",
        "\n",
        "    # Get image ID from image file path\n",
        "    imgStart = imageFile.rfind(\"/\") + 1\n",
        "    imgEnd = imageFile.rfind(\".npy\")\n",
        "    imgID = imageFile[imgStart:imgEnd]\n",
        "\n",
        "    # Search for image in annotations\n",
        "    imgStart = annotations.find(imgID)\n",
        "    if imgStart != -1:\n",
        "      imgStart = annotations.find(\",\", imgStart) + 1\n",
        "\n",
        "      # Get y coordinate\n",
        "      yStart = imgStart\n",
        "      yEnd = annotations.find(\",\", yStart)\n",
        "      y = annotations[yStart:yEnd]\n",
        "\n",
        "      # Get x coordinate\n",
        "      xStart = yEnd + 1\n",
        "      xEnd = annotations.find(\"\\n\", xStart)\n",
        "      x = annotations[xStart:xEnd]\n",
        "\n",
        "      # Append to train images in flattened form to hold in one row\n",
        "      images_train.append(img.flatten())\n",
        "\n",
        "      # Append to train labels\n",
        "      labels_train.append([y,x])\n",
        "\n",
        "      imgIndex += 1\n",
        "  \n",
        "  # Convert lists to np arrays and normalize\n",
        "  x_train = np.array(images_train).astype(np.float32)\n",
        "  y_train = np.array(labels_train).astype(np.float32)\n",
        "\n",
        "  # Comment out section below for augmentation\n",
        "  for i in range (len(y_train)):\n",
        "    x_train[i] = (x_train[i] - np.mean(x_train[i])) / np.std(x_train[i])\n",
        "    y_train[i] /= 511\n",
        "\n",
        "  del images_train\n",
        "  del labels_train\n",
        "\n",
        "  # Print training image count\n",
        "  print(\"Train Total: \" + str(len(y_train)))\n",
        "\n",
        "  return x_train, y_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flybkN5NcP6_"
      },
      "source": [
        "# Show a random set of images and their labels\n",
        "import numpy as np\n",
        "import pylab as py\n",
        "\n",
        "# Create train and test data for the first cross validation fold\n",
        "x_train, y_train, x_test, y_test = create_train_test_data(5, 1)\n",
        "\n",
        "# Print 16 random images\n",
        "fig = py.figure()\n",
        "for i in range(16):\n",
        "  imIndex = np.random.randint(x_train.shape[0])\n",
        "  im = x_train[imIndex].reshape(512, 512)\n",
        "  fig.add_subplot(4, 4, i + 1)\n",
        "  py.imshow(im)\n",
        "  py.axis('off')\n",
        "  print(y_train[imIndex])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMg0SQ9-2D7Y"
      },
      "source": [
        "# Prepare model method\n",
        "def prepare_model(inputs):\n",
        "  # Define kwargs dictionary\n",
        "  kwargs = {\n",
        "      'kernel_size': (3, 3),\n",
        "      'padding': 'same'}\n",
        "\n",
        "  # Define lambda functions\n",
        "  conv = lambda x, filters, strides : layers.Conv2D(filters=filters, strides=strides, **kwargs)(x)\n",
        "  norm = lambda x : layers.BatchNormalization()(x)\n",
        "  relu = lambda x : layers.LeakyReLU()(x)\n",
        "\n",
        "  # Define stride-1, stride-2 blocks\n",
        "  conv1 = lambda filters, x : relu(norm(conv(x, filters, strides=1)))\n",
        "  conv2 = lambda filters, x : relu(norm(conv(x, filters, strides=(2, 2))))\n",
        "\n",
        "  # Define contracting layers\n",
        "  l1 = conv2(48, conv1(48, conv1(48, conv1(48, conv1(48, conv1(48, inputs['dat']))))))\n",
        "  l2 = conv2(56, conv1(56, conv1(56, conv1(56, conv1(56, l1)))))\n",
        "  l3 = conv2(64, conv1(64, conv1(64, conv1(64, conv1(64, l2)))))\n",
        "  l4 = conv2(80, conv1(80, conv1(80, conv1(80, l3))))\n",
        "  l5 = conv2(96, conv1(96, conv1(96, conv1(96, l4))))\n",
        "  l6 = conv2(112, conv1(112, conv1(112, l5)))\n",
        "  l7 = conv2(128, conv1(128, conv1(128, l6)))\n",
        "\n",
        "  # Fully connected layer\n",
        "  fc = layers.Dense(128, activation=\"relu\", name=\"fc\")\n",
        "  l8 = fc(layers.Flatten()(l7))\n",
        "\n",
        "  # Logit layer\n",
        "  c1 = layers.Reshape((1, 1, 1, 128))(l8)\n",
        "  c2 = layers.Conv3D(filters=2, kernel_size=(1, 1, 1), activation='sigmoid')(c1)\n",
        "\n",
        "  # Create logits\n",
        "  logits = {}\n",
        "  logits['car'] = layers.Reshape((-1, 2), name='car')(c2)\n",
        "  \n",
        "  # Create model\n",
        "  model = Model(inputs=inputs, outputs=logits) \n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgWvp4qxlqNj"
      },
      "source": [
        "# Test model method\n",
        "import math\n",
        "\n",
        "def test_model(fold, x_test, y_test):\n",
        "  distInPixelsTest = 0\n",
        "  distInCmTest = 0\n",
        "  totalCnt = len(y_test)\n",
        "  for i in range(totalCnt):\n",
        "    # Predict normalized values of carina coordinates\n",
        "    logits = model.predict(x=x_test[i].reshape(1, 512, 512, 1))\n",
        "\n",
        "    # Denormalize predictions back to coordinates\n",
        "    predY = logits.flatten()[0]*511\n",
        "    predX = logits.flatten()[1]*511\n",
        "    labelY = y_test[i][0]*511\n",
        "    labelX = y_test[i][1]*511\n",
        "    distInPixels = math.sqrt((predY - labelY) ** 2 + (predX - labelX) ** 2)\n",
        "    distInCm = distInPixels * 0.08\n",
        "    distInPixelsTest += distInPixels\n",
        "    distInCmTest += distInCm\n",
        "    with open('carina_512_distances.csv', 'a') as carDistFile:\n",
        "      carDistFile.write(str(fold) + ',' + str(distInPixels) + '\\n')\n",
        "\n",
        "  # Calculate test results\n",
        "  distInPixelsAvg = distInPixelsTest/totalCnt\n",
        "  distInCmAvg = distInCmTest/totalCnt\n",
        "\n",
        "  print('Avg Dist in Pixels: ', distInPixelsAvg)\n",
        "  print('Avg Dist in Cm: ', distInCmAvg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ti_57v1QdcN"
      },
      "source": [
        "#--------------------------------------------\n",
        "# Train and test with 5-fold cross validation\n",
        "#--------------------------------------------\n",
        "\n",
        "import datetime\n",
        "\n",
        "with open('carina_512_distances.csv', 'a') as carDistFile:\n",
        "  carDistFile.write('fold,dist\\n')\n",
        "\n",
        "for fold in range(5):\n",
        "  print('fold: ' + str(fold))\n",
        "\n",
        "  # Create model inputs\n",
        "  inputs = {}\n",
        "  inputs['dat'] = Input(shape=(512, 512, 1))\n",
        "\n",
        "  # Prepare the model\n",
        "  model = prepare_model(inputs)\n",
        "\n",
        "  # Create train and test data for the cross validation fold\n",
        "  x_train, y_train, x_test, y_test = create_train_test_data(5, fold + 1)\n",
        "\n",
        "  # Initialize learning rate and epoch\n",
        "  lr = 0.0005\n",
        "  epoch = 1\n",
        "\n",
        "  for j in range(5):\n",
        "    # Compile the model\n",
        "    model.compile(\n",
        "        optimizer=optimizers.Adam(learning_rate=lr),\n",
        "        loss={'car': losses.MeanSquaredError()})\n",
        "\n",
        "    for i in range(2):\n",
        "      # Print learning-rate and epoch\n",
        "      print('learning-rate: ' + str(lr))\n",
        "      print('epoch: ' + str(epoch))\n",
        "\n",
        "      # Train the model\n",
        "      model.fit(\n",
        "          x=x_train.reshape(x_train.shape[0], 512, 512, 1), \n",
        "          y=y_train,\n",
        "          batch_size=1,\n",
        "          steps_per_epoch=None, \n",
        "          epochs=1)\n",
        "\n",
        "      # Increment epoch\n",
        "      epoch += 1\n",
        "\n",
        "      # Check epoch\n",
        "      if epoch == 10:\n",
        "        break\n",
        "\n",
        "    # Check epoch\n",
        "    if epoch == 10:\n",
        "      break\n",
        "    else:\n",
        "      lr /= 2\n",
        "\n",
        "  # Set start time\n",
        "  startTime = datetime.datetime.now().time()\n",
        "\n",
        "  # Test the model\n",
        "  test_model(fold, x_test, y_test)\n",
        "\n",
        "  # Set end time\n",
        "  endTime = datetime.datetime.now().time()\n",
        "\n",
        "  # Print start and end times\n",
        "  print(\"Start Time: \" + str(startTime))\n",
        "  print(\"End Time: \" + str(endTime))\n",
        "  print(\"Item Count: \" + str(len(y_test)))\n",
        "\n",
        "  # Delete the model at the end of each fold\n",
        "  del model\n",
        "  del x_train\n",
        "  del y_train\n",
        "  del x_test\n",
        "  del y_test\n",
        "\n",
        "# Copy the error distance file to google file\n",
        "!cp ./carina_512_distances.csv '/content/drive/My Drive/Colab Notebooks/carina_512_7396_distances.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7G99Icq9LG7Q"
      },
      "source": [
        "#--------------------------------------------------\n",
        "# Generate a model file from the whole training set\n",
        "#--------------------------------------------------\n",
        "\n",
        "# Create model inputs\n",
        "inputs = {}\n",
        "inputs['dat'] = Input(shape=(512, 512, 1))\n",
        "\n",
        "# Prepare the model\n",
        "model = prepare_model(inputs)\n",
        "\n",
        "# Create train data to build a model\n",
        "x_train, y_train = create_train_data()\n",
        "\n",
        "# Initialize learning rate and epoch\n",
        "lr = 0.0005\n",
        "epoch = 1\n",
        "\n",
        "for j in range(5):\n",
        "  # Compile the model\n",
        "  model.compile(\n",
        "      optimizer=optimizers.Adam(learning_rate=lr),\n",
        "      loss={'car': losses.MeanSquaredError()})\n",
        "\n",
        "  for i in range(2):\n",
        "    # Print learning-rate and epoch\n",
        "    print('learning-rate: ' + str(lr))\n",
        "    print('epoch: ' + str(epoch))\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(\n",
        "        x=x_train.reshape(x_train.shape[0], 512, 512, 1), \n",
        "        y=y_train,\n",
        "        batch_size=1,\n",
        "        steps_per_epoch=None, \n",
        "        epochs=1)\n",
        "\n",
        "    # Increment epoch\n",
        "    epoch += 1\n",
        "\n",
        "    # Check epoch\n",
        "    if epoch == 10:\n",
        "      break\n",
        "\n",
        "  # Check epoch\n",
        "  if epoch == 10:\n",
        "    break\n",
        "  else:\n",
        "    lr /= 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyPSFBm2LfHu"
      },
      "source": [
        "# --- Save the model to a file\n",
        "model.save('./cnn_1_carina_regression.hdf5')\n",
        "\n",
        "# Copy the model to google drive\n",
        "!cp ./cnn_1_carina_regression.hdf5 '/content/drive/My Drive/Colab Notebooks/cnn_1_carina_regression.hdf5'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcxWdoebxXOQ"
      },
      "source": [
        "# Copy the model file from google drive\n",
        "!cp '/content/drive/My Drive/Colab Notebooks/cnn_1_carina_regression.hdf5' ./cnn_1_carina_regression.hdf5\n",
        "\n",
        "# Load the model from the file\n",
        "from tensorflow.keras import models as tfModels\n",
        "model = tfModels.load_model('./cnn_1_carina_regression.hdf5', compile=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsLYifLqiaZZ"
      },
      "source": [
        "def crop_image(imgS, hS, wS, yC, xC, hT, wT):\n",
        "  # Carina must be at (.5, .75) in terms of (x, y)\n",
        "  cL = wT // 2\n",
        "  cR = cL\n",
        "  cT = hT * 3 // 4\n",
        "  cB = hT - cT\n",
        "\n",
        "  # Calculate left and right x coordinates, top and bottom y coordinates of\n",
        "  # the 256x128 cropped area around carina on the original 512x512 image\n",
        "  xL = xC - cL\n",
        "  xR = xC + cR\n",
        "  yT = yC - cT\n",
        "  yB = yC + cB\n",
        "\n",
        "  # Calculate width by checking 3 possible cases of xL and xR\n",
        "  if xL < 0:\n",
        "    w = wT - abs(xL)\n",
        "  elif xR > (wS - 1):\n",
        "    w = wT - abs(xR - (wS - 1))\n",
        "  else:\n",
        "    w = wT\n",
        "\n",
        "  # Calculate height by checking 3 possible cases of yT and yB \n",
        "  if yT < 0:\n",
        "    h = hT - abs(yT)\n",
        "  elif yB > (hS - 1):\n",
        "    h = hT - abs(yB - (hS - 1))\n",
        "  else:\n",
        "    h = hT\n",
        "\n",
        "  # Calculate x0 on original  \n",
        "  if xL < 0:\n",
        "    x0 = 0\n",
        "  else:\n",
        "    x0 = xL\n",
        "\n",
        "  # Calculate y0 on original  \n",
        "  if yT < 0:\n",
        "    y0 = 0\n",
        "  else:\n",
        "    y0 = yT\n",
        "\n",
        "  # Calculate x1 on target  \n",
        "  if xL < 0:\n",
        "    x1 = abs(xL)\n",
        "  else:\n",
        "    x1 = 0\n",
        "\n",
        "  # Calculate y1 on target  \n",
        "  if yT < 0:\n",
        "    y1 = abs(yT)\n",
        "  else:\n",
        "    y1 = 0\n",
        "\n",
        "  # Create an empty target image\n",
        "  imgT = np.zeros((hT, wT), np.uint16)\n",
        "\n",
        "  # Copy the cropped part of the image from source to target\n",
        "  np.copyto(imgT[y1:, x1:], imgS[y0:y0+h, x0:x0+w])\n",
        "\n",
        "  return imgT"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKeI0-T6ibfT"
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "\n",
        "# Use the model to find the carina in all images under npys, and\n",
        "# crop an area of 256x128 around the carina to have it located at [.75, .5]\n",
        "def crop_images_around_carina(hS, wS, hT, wT):\n",
        "  # Read the contents of the frontal file\n",
        "  with open(\"./db-mimic-frontal-only.csv\", 'r') as frontalFile:\n",
        "    frontalContent = frontalFile.read()\n",
        "\n",
        "  # Read the contents of the annotations file\n",
        "  with open(\"./new_carina_annotations_7396.csv\", 'r') as annotationsFile:\n",
        "    annotations = annotationsFile.read()\n",
        "\n",
        "  # Search for all image files recursively\n",
        "  imgFiles = glob.glob('./npys/**/*.npy', recursive=True)\n",
        "\n",
        "  # Process each image file\n",
        "  for imgFile in imgFiles:\n",
        "    # Get image name\n",
        "    imgStart = imgFile.rfind(\"/\") + 1\n",
        "    imgEnd = imgFile.rfind(\".npy\")\n",
        "    imgName = imgFile[imgStart:imgEnd]\n",
        "\n",
        "    # Skip the image if it's not frontal\n",
        "    if frontalContent.find(imgName) == -1:\n",
        "      continue\n",
        "\n",
        "    # Load an original image such as 512xN or Nx512\n",
        "    imgO = np.load(imgFile)\n",
        "\n",
        "    # Create an empty source image\n",
        "    imgS = np.zeros((hS, wS), np.uint16)\n",
        "\n",
        "    # Insert the original image to the top left corner of the source image\n",
        "    imgS[0:imgO.shape[0], 0:imgO.shape[1]] = imgO\n",
        "    \n",
        "    # Initialize x and y coordinates\n",
        "    x = wS / 2\n",
        "    y = hS * 3 / 4\n",
        "\n",
        "    # Search for image in annotations\n",
        "    antStart = annotations.find(imgName)\n",
        "    if antStart != -1:\n",
        "      # Get y coordinate\n",
        "      yStart = annotations.find(\",\", antStart) + 1\n",
        "      yEnd = annotations.find(\",\", yStart)\n",
        "      y = int(float(annotations[yStart:yEnd]))\n",
        "\n",
        "      # Get x coordinate\n",
        "      xStart = yEnd + 1\n",
        "      xEnd = annotations.find(\"\\n\", xStart)\n",
        "      x = int(float(annotations[xStart:xEnd]))\n",
        "    else:\n",
        "      # Normalize the source image\n",
        "      imgN = (imgS - np.mean(imgS)) / np.std(imgS)\n",
        "\n",
        "      # Predict coordinates of carina\n",
        "      logits = model.predict(imgN.reshape(1, hS, wS, 1))\n",
        "\n",
        "      # Denormalize predictions back to coordinates\n",
        "      y = int(logits.flatten()[0]*511)\n",
        "      x = int(logits.flatten()[1]*511)\n",
        "\n",
        "    # Crop image around carina\n",
        "    croppedImg = crop_image(imgS, hS, wS, y, x, hT, wT)\n",
        "\n",
        "    # Save image at its new directory\n",
        "    croppedImgFile = imgFile.replace(\"npys\", \"crops\")\n",
        "    directory = os.path.dirname(croppedImgFile)\n",
        "    if not os.path.exists(directory):\n",
        "      os.makedirs(directory)\n",
        "    np.save(croppedImgFile, croppedImg)\n",
        "\n",
        "# Crop images around carina\n",
        "crop_images_around_carina(512, 512, 256, 128)\n",
        "\n",
        "# Zip the crops folder\n",
        "!zip -qr crops.zip ./crops\n",
        "\n",
        "# Cop the zip file to google drive\n",
        "!cp crops.zip \"/content/drive/My Drive/Colab Notebooks/crops.zip\"\n",
        "\n",
        "# Print the number of cropped images\n",
        "print(len(glob.glob('./crops/**/*.npy', recursive=True)))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}